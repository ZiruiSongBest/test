#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“

åŸºäºBAGELæ¨¡å‹çš„å›¾åƒç¼–è¾‘æ¨ç†ï¼Œæ­£ç¡®çš„æ¨¡å‹åŠ è½½æ–¹å¼å‚è€ƒinferencer.py
ä¸“æ³¨äºå›¾åƒç¼–è¾‘æ¨¡å¼çš„æ¨ç†ï¼Œä¸åŒ…å«å¤æ‚çš„è‡ªå›å½’äº¤é”™ç”Ÿæˆé€»è¾‘ã€‚
"""

import os
import sys
import json
import torch
from pathlib import Path
from typing import List, Dict, Union, Optional, Tuple, Any
from PIL import Image
import numpy as np
import logging
import argparse
from copy import deepcopy

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(current_dir))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from data.data_utils import add_special_tokens, pil_img2rgb
from data.transforms import ImageTransform
from modeling.autoencoder import load_ae
from modeling.bagel import (
    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,
    SiglipVisionConfig, SiglipVisionModel
)
from modeling.qwen2 import Qwen2Tokenizer
from inferencer import InterleaveInferencer
from modeling.bagel.qwen2_navit import NaiveCache

# å¦‚æœéœ€è¦ä½¿ç”¨é‡åŒ–æˆ–å¤šGPU
from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model
from safetensors.torch import load_file

logger = logging.getLogger(__name__)


class UnifiedAutoregressiveInferencer:
    """
    ç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨ - çœŸæ­£å¯¹åº”è®­ç»ƒæ—¶çš„forward_autoregressive_trainingé€»è¾‘
    
    ä¸è®­ç»ƒçš„ä¸»è¦å¯¹åº”å…³ç³»ï¼š
    1. ç»Ÿä¸€åºåˆ—å»ºæ¨¡ï¼šæ–‡æœ¬token + ç‰¹æ®Štoken + å›¾åƒpatches
    2. é€tokenè‡ªå›å½’ç”Ÿæˆï¼ŒåŒ…æ‹¬ç‰¹æ®Štokené¢„æµ‹
    3. patchçº§åˆ«çš„å›¾åƒFlow Matchingç”Ÿæˆ
    4. ä¿æŒè®­ç»ƒæ¨ç†ä¸€è‡´çš„æ•°æ®æµ
    """
    
    def __init__(self, model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids):
        self.model = model
        self.vae_model = vae_model
        self.tokenizer = tokenizer
        self.vae_transform = vae_transform
        self.vit_transform = vit_transform
        self.new_token_ids = new_token_ids
        self.device = next(model.parameters()).device
        
        # ç‰¹æ®Štoken IDs
        self.start_of_image = new_token_ids.get('start_of_image')
        self.end_of_image = new_token_ids.get('end_of_image')
        self.bos_token_id = new_token_ids.get('bos_token_id')
        self.eos_token_id = new_token_ids.get('eos_token_id')
        
        print(f"ğŸ”§ ç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - start_of_image: {self.start_of_image}")
        print(f"   - end_of_image: {self.end_of_image}")
    
    @torch.no_grad()
    def unified_autoregressive_inference(
        self,
        input_text: str,
        input_image: Optional[Image.Image] = None,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 1.0,
        image_shapes: tuple = (1024, 1024),
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        timestep_shift: float = 3.0,
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        ç»Ÿä¸€çš„è‡ªå›å½’æ¨ç†ï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        Args:
            input_text: è¾“å…¥æ–‡æœ¬ï¼ˆå¯¹åº”è®­ç»ƒæ—¶çš„input_textï¼‰
            input_image: è¾“å…¥å›¾åƒï¼ˆå¯¹åº”è®­ç»ƒæ—¶çš„input_imageï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: æ¸©åº¦å‚æ•°
            image_shapes: ç”Ÿæˆå›¾åƒå°ºå¯¸
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: Flow Matchingæ­¥æ•°
            timestep_shift: æ—¶é—´æ­¥åç§»
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆæ–‡æœ¬å’Œå›¾åƒäº¤é”™ï¼‰
        """
        print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€è‡ªå›å½’æ¨ç†")
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {input_text}")
        print(f"ğŸ–¼ï¸  è¾“å…¥å›¾åƒ: {input_image.size if input_image else 'None'}")
        
        # 1. å¤„ç†è¾“å…¥é˜¶æ®µï¼ˆå¯¹åº”è®­ç»ƒçš„_process_input_stageï¼‰
        input_embeddings, input_sequence_length = self._process_input_stage(
            input_text, input_image
        )
        
        # 2. ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼ˆå¯¹åº”è®­ç»ƒçš„_process_unified_autoregressive_trainingï¼‰
        generated_sequence = self._unified_autoregressive_generation(
            input_embeddings=input_embeddings,
            input_sequence_length=input_sequence_length,
            max_length=max_length,
            do_sample=do_sample,
            temperature=temperature,
            image_shapes=image_shapes,
            cfg_text_scale=cfg_text_scale,
            cfg_img_scale=cfg_img_scale,
            num_timesteps=num_timesteps,
            timestep_shift=timestep_shift,
            **kwargs
        )
        
        # 3. è§£æè¾“å‡ºåºåˆ—
        output_results = self._parse_generated_sequence(generated_sequence)
        
        print(f"âœ… ç»Ÿä¸€è‡ªå›å½’æ¨ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(output_results)} ä¸ªç»“æœ")
        return output_results
    
    def _process_input_stage(
        self, 
        input_text: str, 
        input_image: Optional[Image.Image]
    ) -> Tuple[torch.Tensor, int]:
        """
        å¤„ç†è¾“å…¥é˜¶æ®µï¼Œå¯¹åº”è®­ç»ƒæ—¶çš„_process_input_stage
        
        è®­ç»ƒæ—¶çš„é¡ºåºï¼šæ–‡æœ¬åœ¨å‰ï¼Œå›¾åƒåœ¨å
        
        Returns:
            (input_embeddings, sequence_length)
        """
        sequence_parts = []
        total_length = 0
        
        # 1. å¤„ç†è¾“å…¥æ–‡æœ¬ - ä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼Œæ–‡æœ¬åœ¨å‰
        text_ids = self.tokenizer.encode(input_text)
        text_embedding = self.model.language_model.model.embed_tokens(
            torch.tensor(text_ids, device=self.device)
        )
        
        sequence_parts.append(text_embedding)
        total_length += len(text_ids)
        
        # 2. å¤„ç†è¾“å…¥å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰- å›¾åƒåœ¨æ–‡æœ¬ä¹‹å
        if input_image is not None:
            # è½¬æ¢å›¾åƒæ ¼å¼
            from data.data_utils import pil_img2rgb, patchify
            
            if input_image.mode != 'RGB':
                input_image = input_image.convert('RGB')
            
            # ä½¿ç”¨VITå˜æ¢å¤„ç†å›¾åƒ
            image_tensor = self.vit_transform(pil_img2rgb(input_image))
            if image_tensor.dim() == 3:
                image_tensor = image_tensor.unsqueeze(0)
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸æ¨¡å‹æƒé‡ä¸€è‡´
            model_dtype = next(self.model.parameters()).dtype
            image_tensor = image_tensor.to(dtype=model_dtype, device=self.device)
            
            # ä½¿ç”¨ViTå¤„ç†è¾“å…¥å›¾åƒï¼ˆå¯¹åº”è®­ç»ƒé€»è¾‘ï¼‰
            vit_position_ids = self.model.get_flattened_position_ids(
                image_tensor.size(2), image_tensor.size(3),
                self.model.vit_patch_size,
                max_num_patches_per_side=self.model.vit_max_num_patch_per_side
            ).to(self.device)
            
            vit_tokens = patchify(image_tensor.squeeze(0), self.model.vit_patch_size)
            
            # ä½¿ç”¨ViTæ¨¡å‹å¤„ç†
            cu_seqlens = torch.tensor([0, vit_tokens.shape[0]], dtype=torch.int32, device=self.device)
            vit_embeddings = self.model.vit_model(
                packed_pixel_values=vit_tokens,
                packed_flattened_position_ids=vit_position_ids,
                cu_seqlens=cu_seqlens,
                max_seqlen=vit_tokens.shape[0],
            )
            
            # åº”ç”¨è¿æ¥å™¨å’Œä½ç½®ç¼–ç 
            vit_embeddings = self.model.connector(vit_embeddings)
            vit_pos_emb = self.model.vit_pos_embed(vit_position_ids)
            vit_embeddings = vit_embeddings + vit_pos_emb
            
            sequence_parts.append(vit_embeddings)
            total_length += len(vit_embeddings)
        
        # 3. æ„å»ºåˆå§‹åºåˆ—ï¼ˆä¸è®­ç»ƒæ—¶çš„é¡ºåºä¸€è‡´ï¼štext_embedding + vit_embeddingsï¼‰
        input_embeddings = torch.cat(sequence_parts, dim=0)
        
        print(f"âœ… è¾“å…¥å¤„ç†å®Œæˆï¼Œåºåˆ—é•¿åº¦: {total_length}")
        print(f"ğŸ“„ æ–‡æœ¬tokens: {len(text_ids)}")
        if input_image is not None:
            print(f"ğŸ–¼ï¸  å›¾åƒpatches: {len(vit_embeddings)}")
        return input_embeddings, total_length
    
    def _unified_autoregressive_generation(
        self,
        input_embeddings: torch.Tensor,
        input_sequence_length: int,
        max_length: int,
        do_sample: bool,
        temperature: float,
        image_shapes: tuple,
        cfg_text_scale: float,
        cfg_img_scale: float,
        num_timesteps: int,
        timestep_shift: float,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€tokenå¤„ç†é€»è¾‘
        
        è®­ç»ƒæ—¶çš„æ ¸å¿ƒé€»è¾‘ï¼š
        1. é€tokené¢„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬tokenå’Œç‰¹æ®Štokenï¼ˆ<vision_start>, <vision_end>ï¼‰
        2. æ¨¡å‹è‡ªå·±å†³å®šä½•æ—¶è¾“å‡º<vision_start>å¼€å§‹å›¾åƒç”Ÿæˆ
        3. åœ¨<vision_start>å’Œ<vision_end>ä¹‹é—´ï¼Œé€patchç”Ÿæˆå›¾åƒ
        4. æ¨¡å‹è‡ªå·±å†³å®šä½•æ—¶è¾“å‡º<vision_end>ç»“æŸå›¾åƒç”Ÿæˆ
        
        Returns:
            ç”Ÿæˆåºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«tokenç±»å‹å’Œå†…å®¹
        """
        print(f"ğŸ¯ å¼€å§‹ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼Œæœ€å¤§é•¿åº¦: {max_length}")
        
        # å½“å‰åºåˆ—çŠ¶æ€
        current_embeddings = input_embeddings.clone()
        generated_sequence = []
        step = 0
        
        # ç”ŸæˆçŠ¶æ€è·Ÿè¸ª
        in_image_generation = False
        current_image_patches = []
        current_image_shape = None
        patches_generated_in_current_image = 0
        max_patches_per_image = None
        
        # åˆå§‹åŒ– past_key_values å’Œç›¸å…³å‚æ•°
        past_key_values = NaiveCache(self.model.config.llm_config.num_hidden_layers)
        
        while step < max_length:
            # è®¡ç®—å½“å‰æ­¥éª¤çš„KV cacheå‚æ•°
            if step == 0:
                # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å®Œæ•´è¾“å…¥åºåˆ—ï¼Œæ²¡æœ‰past key values
                kv_lens = [0]
                kv_indexes = torch.tensor([], dtype=torch.long, device=self.device)
            else:
                # åç»­æ­¥éª¤ï¼šæœ‰past key values
                kv_lens = [len(current_embeddings) - 1]  # paståºåˆ—é•¿åº¦
                kv_indexes = torch.arange(len(current_embeddings) - 1, device=self.device)
            # 1. é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆå¯èƒ½æ˜¯æ–‡æœ¬ã€ç‰¹æ®Štokenæˆ–å›¾åƒpatchï¼‰
            if step == 0:
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å®Œæ•´è¾“å…¥åºåˆ—
                query_embeddings = current_embeddings
            else:
                # åç»­æ­¥éª¤ï¼šåªä½¿ç”¨æœ€åä¸€ä¸ªtoken
                query_embeddings = current_embeddings[-1:, :]
                
            next_token_info = self._predict_next_token_unified(
                query_embeddings, 
                in_image_generation,
                patches_generated_in_current_image,
                max_patches_per_image,
                do_sample, 
                temperature,
                past_key_values,
                kv_lens,
                kv_indexes
            )
            
            token_id = next_token_info.get('token_id')
            token_type = next_token_info.get('token_type')
            
            print(f"ç¬¬ {step+1} æ­¥: é¢„æµ‹ {token_type}, token_id: {token_id}")
            if token_type == 'special':
                if token_id == self.start_of_image:
                    print(f"   -> é¢„æµ‹åˆ° start_of_image")
                elif token_id == self.end_of_image:
                    print(f"   -> é¢„æµ‹åˆ° end_of_image")
                elif token_id == self.eos_token_id:
                    print(f"   -> é¢„æµ‹åˆ° EOS")
                else:
                    print(f"   -> é¢„æµ‹åˆ°å…¶ä»–ç‰¹æ®Štoken: {token_id}")
            
            # 2. æ ¹æ®é¢„æµ‹ç»“æœå¤„ç†
            if token_type == 'text':
                # æ™®é€šæ–‡æœ¬token
                token_embedding = self.model.language_model.model.embed_tokens(
                    torch.tensor([token_id], device=self.device)
                )
                current_embeddings = torch.cat([current_embeddings, token_embedding], dim=0)
                
                generated_sequence.append({
                    'type': 'text_token',
                    'content': token_id
                })
                
            elif token_type == 'special' and token_id == self.start_of_image:
                # æ¨¡å‹é¢„æµ‹è¦å¼€å§‹ç”Ÿæˆå›¾åƒ
                print(f"ğŸ–¼ï¸  æ¨¡å‹å†³å®šå¼€å§‹å›¾åƒç”Ÿæˆ")
                in_image_generation = True
                current_image_shape = image_shapes
                patches_generated_in_current_image = 0
                
                # è®¡ç®—å½“å‰å›¾åƒçš„æœ€å¤§patchæ•°
                H, W = image_shapes
                h = H // self.model.latent_downsample
                w = W // self.model.latent_downsample
                max_patches_per_image = h * w
                
                # æ·»åŠ <vision_start> tokenåˆ°åºåˆ—
                token_embedding = self.model.language_model.model.embed_tokens(
                    torch.tensor([token_id], device=self.device)
                )
                current_embeddings = torch.cat([current_embeddings, token_embedding], dim=0)
                
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'start_of_image'
                })
                
            elif token_type == 'special' and token_id == self.end_of_image:
                # æ¨¡å‹é¢„æµ‹è¦ç»“æŸå›¾åƒç”Ÿæˆ
                print(f"ğŸ–¼ï¸  æ¨¡å‹å†³å®šç»“æŸå›¾åƒç”Ÿæˆï¼Œå·²ç”Ÿæˆ {patches_generated_in_current_image} ä¸ªpatches")
                
                # å®Œæˆå›¾åƒç”Ÿæˆ
                if current_image_patches:
                    generated_image = self._finalize_image_generation(
                        current_image_patches, current_image_shape,
                        cfg_text_scale, cfg_img_scale, num_timesteps, timestep_shift
                    )
                    generated_sequence.append({
                        'type': 'image',
                        'content': generated_image
                    })
                    current_image_patches = []
                
                in_image_generation = False
                patches_generated_in_current_image = 0
                max_patches_per_image = None
                
                # æ·»åŠ <vision_end> tokenåˆ°åºåˆ—
                token_embedding = self.model.language_model.model.embed_tokens(
                    torch.tensor([token_id], device=self.device)
                )
                current_embeddings = torch.cat([current_embeddings, token_embedding], dim=0)
                
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'end_of_image'
                })
                
            elif token_type == 'image_patch':
                # ç”Ÿæˆå›¾åƒpatchï¼ˆå¯¹åº”è®­ç»ƒæ—¶çš„flow matchingï¼‰
                patch_data = self._generate_image_patch_unified(
                    current_embeddings, 
                    current_image_shape,
                    patches_generated_in_current_image,
                    cfg_text_scale,
                    cfg_img_scale,
                    past_key_values,
                    kv_lens,
                    kv_indexes
                )
                current_image_patches.append(patch_data)
                patches_generated_in_current_image += 1
                
                # æ·»åŠ patch embeddingåˆ°åºåˆ—
                current_embeddings = torch.cat([
                    current_embeddings, 
                    patch_data['embedding']
                ], dim=0)
                
                generated_sequence.append({
                    'type': 'image_patch',
                    'content': patch_data
                })
                
                # æ£€æŸ¥æ˜¯å¦å·²ç”Ÿæˆæ‰€æœ‰patches
                if patches_generated_in_current_image >= max_patches_per_image:
                    print(f"ğŸ“Š å·²ç”Ÿæˆæ‰€æœ‰ {max_patches_per_image} ä¸ªpatches")
                    # ä¸‹ä¸€æ­¥åº”è¯¥é¢„æµ‹<vision_end>
                
            elif token_id == self.eos_token_id:
                # ç»“æŸç”Ÿæˆ
                print(f"ğŸ é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'eos'
                })
                break
                
            else:
                # å…¶ä»–ç‰¹æ®Štoken
                token_embedding = self.model.language_model.model.embed_tokens(
                    torch.tensor([token_id], device=self.device)
                )
                current_embeddings = torch.cat([current_embeddings, token_embedding], dim=0)
                
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id
                })
            
            step += 1
        
        # å¤„ç†æœªå®Œæˆçš„å›¾åƒç”Ÿæˆ
        if in_image_generation and current_image_patches:
            print(f"âš ï¸  ç”Ÿæˆæœªå®Œæˆï¼Œå¼ºåˆ¶å®Œæˆå›¾åƒç”Ÿæˆ")
            generated_image = self._finalize_image_generation(
                current_image_patches, current_image_shape,
                cfg_text_scale, cfg_img_scale, num_timesteps, timestep_shift
            )
            generated_sequence.append({
                'type': 'image',
                'content': generated_image
            })
        
        print(f"âœ… è‡ªå›å½’ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(generated_sequence)} ä¸ªå…ƒç´ ")
        return generated_sequence
    
    def _predict_next_token_unified(
        self, 
        current_embeddings: torch.Tensor,
        in_image_generation: bool,
        patches_generated: int,
        max_patches: Optional[int],
        do_sample: bool, 
        temperature: float,
        past_key_values: NaiveCache,
        kv_lens: List[int],
        kv_indexes: torch.Tensor
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        å…³é”®ç‚¹ï¼š
        1. å¦‚æœä¸åœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œé¢„æµ‹æ–‡æœ¬tokenæˆ–ç‰¹æ®Štokenï¼ˆåŒ…æ‹¬<vision_start>ï¼‰
        2. å¦‚æœåœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œç”Ÿæˆå›¾åƒpatchç›´åˆ°å®Œæˆï¼Œç„¶åé¢„æµ‹<vision_end>
        
        Args:
            current_embeddings: å½“å‰åºåˆ—çš„embeddings
            in_image_generation: æ˜¯å¦åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­
            patches_generated: å½“å‰å›¾åƒå·²ç”Ÿæˆçš„patchæ•°
            max_patches: å½“å‰å›¾åƒçš„æœ€å¤§patchæ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        # æ„å»ºä½ç½®ID
        seq_len = current_embeddings.size(0)
        position_ids = torch.arange(seq_len, device=self.device)
        
        # LLMå‰å‘ä¼ æ’­
        output = self.model.language_model(
            packed_query_sequence=current_embeddings.unsqueeze(0),
            query_lens=torch.tensor([seq_len], device=self.device),
            packed_query_position_ids=position_ids,
            packed_query_indexes=torch.arange(seq_len, device=self.device),
            past_key_values=past_key_values,
            key_values_lens=torch.tensor(kv_lens, device=self.device),
            packed_key_value_indexes=kv_indexes,
            update_past_key_values=True,
            is_causal=True,
            mode="und"
        )
        
        # è·å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        # æ ¹æ®qwen2_navit.pyçš„BaseNavitOutputWithPastï¼Œéœ€è¦è·å–packed_query_sequence
        hidden_states = output.packed_query_sequence
            
        last_hidden_state = hidden_states[0, -1, :]
        
        if not in_image_generation:
            # ä¸åœ¨å›¾åƒç”Ÿæˆä¸­ï¼šé¢„æµ‹æ–‡æœ¬tokenæˆ–ç‰¹æ®Štoken
            logits = self.model.language_model.lm_head(last_hidden_state.unsqueeze(0))
            
            if do_sample:
                import torch.nn.functional as F
                probs = F.softmax(logits / temperature, dim=-1)
                next_token = torch.multinomial(probs.squeeze(0), num_samples=1)
            else:
                next_token = torch.argmax(logits, dim=-1)
            
            token_id = next_token.item()
            
            # åˆ¤æ–­tokenç±»å‹
            if token_id == self.start_of_image:
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            elif token_id == self.end_of_image:
                # ä¸åº”è¯¥åœ¨éå›¾åƒç”Ÿæˆæ—¶é¢„æµ‹åˆ°end_of_image
                # ä½†ä¸ºäº†é²æ£’æ€§è¿˜æ˜¯å¤„ç†
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            elif token_id == self.eos_token_id:
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            else:
                return {
                    'token_id': token_id,
                    'token_type': 'text',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
        else:
            # åœ¨å›¾åƒç”Ÿæˆä¸­
            if patches_generated < max_patches:
                # è¿˜éœ€è¦ç”Ÿæˆæ›´å¤špatches
                return {
                    'token_type': 'image_patch',
                    'hidden_state': last_hidden_state
                }
            else:
                # å·²ç”Ÿæˆæ‰€æœ‰patchesï¼Œåº”è¯¥é¢„æµ‹<vision_end>
                # å¼ºåˆ¶é¢„æµ‹<vision_end>æˆ–è®©æ¨¡å‹è‡ªå·±å†³å®š
                logits = self.model.language_model.lm_head(last_hidden_state.unsqueeze(0))
                
                # è¿™é‡Œå¯ä»¥é€‰æ‹©ï¼š
                # 1. å¼ºåˆ¶è¾“å‡º<vision_end>
                # 2. è®©æ¨¡å‹è‡ªå·±é¢„æµ‹ï¼ˆå¯èƒ½ç»§ç»­ç”Ÿæˆæˆ–ç»“æŸï¼‰
                
                # æ–¹æ¡ˆ1ï¼šå¼ºåˆ¶è¾“å‡ºï¼ˆæ›´ç¨³å®šï¼‰
                return {
                    'token_id': self.end_of_image,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
                
                # æ–¹æ¡ˆ2ï¼šè®©æ¨¡å‹å†³å®šï¼ˆæ³¨é‡Šæ‰çš„ä»£ç ï¼‰
                # if do_sample:
                #     import torch.nn.functional as F
                #     probs = F.softmax(logits / temperature, dim=-1)
                #     next_token = torch.multinomial(probs.squeeze(0), num_samples=1)
                # else:
                #     next_token = torch.argmax(logits, dim=-1)
                # 
                # token_id = next_token.item()
                # if token_id == self.end_of_image:
                #     return {
                #         'token_id': token_id,
                #         'token_type': 'special',
                #         'logits': logits,
                #         'hidden_state': last_hidden_state
                #     }
                # else:
                #     # ç»§ç»­ç”Ÿæˆpatchï¼ˆå¯èƒ½è¶…è¿‡é¢„æœŸæ•°é‡ï¼‰
                #     return {
                #         'token_type': 'image_patch',
                #         'hidden_state': last_hidden_state
                #     }
    
    def _generate_image_patch_unified(
        self, 
        current_embeddings: torch.Tensor, 
        image_shape: tuple, 
        patch_index: int,
        cfg_text_scale: float,
        cfg_img_scale: float,
        past_key_values: NaiveCache,
        kv_lens: List[int],
        kv_indexes: torch.Tensor,
        num_flow_steps: int = 10
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•ä¸ªå›¾åƒpatchï¼Œä½¿ç”¨Flow Matchingï¼Œå¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        ä¸è®­ç»ƒçš„å¯¹åº”å…³ç³»ï¼š
        1. è®­ç»ƒæ—¶ï¼šç»™å®šnoisy patchï¼Œé¢„æµ‹velocity (noise - clean)
        2. æ¨ç†æ—¶ï¼šä»çº¯å™ªå£°å¼€å§‹ï¼Œé€šè¿‡é¢„æµ‹çš„velocityé€æ­¥å»å™ª
        
        Args:
            current_embeddings: å½“å‰åºåˆ—çš„embeddings
            image_shape: å›¾åƒå°ºå¯¸
            patch_index: å½“å‰patchçš„ç´¢å¼•
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦  
            num_flow_steps: Flow Matchingçš„å»å™ªæ­¥æ•°
            
        Returns:
            åŒ…å«patchä¿¡æ¯çš„å­—å…¸
        """
        # è®¡ç®—patchçš„ä½ç½®å’Œå°ºå¯¸ä¿¡æ¯
        H, W = image_shape
        h = H // self.model.latent_downsample
        w = W // self.model.latent_downsample
        
        # ç”Ÿæˆä½ç½®ID
        patch_position_ids = self.model.get_flattened_position_ids(
            h * self.model.latent_downsample, w * self.model.latent_downsample,
            self.model.latent_downsample,
            max_num_patches_per_side=self.model.max_latent_size
        ).to(self.device)
        
        current_patch_pos_id = patch_position_ids[patch_index:patch_index+1]
        
        # åˆå§‹åŒ–ï¼šä»çº¯å™ªå£°å¼€å§‹
        patch_dim = self.model.latent_patch_size ** 2 * self.model.latent_channel
        x_t = torch.randn(1, patch_dim, device=self.device)  # çº¯å™ªå£°
        
        # Flow Matchingå»å™ªè¿‡ç¨‹
        timesteps = torch.linspace(1, 0, num_flow_steps, device=self.device)
        for t in timesteps:
            # å‡†å¤‡æ—¶é—´æ­¥
            timestep = torch.tensor([t], device=self.device)
            timestep_processed = torch.sigmoid(timestep)
            timestep_processed = self.model.timestep_shift * timestep_processed / (1 + (self.model.timestep_shift - 1) * timestep_processed)
            
            # æ„å»ºå½“å‰patchçš„embedding
            timestep_embed = self.model.time_embedder(timestep)
            latent_pos_embed = self.model.latent_pos_embed(current_patch_pos_id)
            patch_embedding = self.model.vae2llm(x_t) + timestep_embed + latent_pos_embed
            
            # å°†patch embeddingæ·»åŠ åˆ°å½“å‰åºåˆ—æœ«å°¾ï¼ˆä¸´æ—¶ï¼‰
            temp_embeddings = torch.cat([current_embeddings, patch_embedding.squeeze(0)], dim=0)
            
            # æ„å»ºä½ç½®ID
            seq_len = temp_embeddings.size(0)
            position_ids = torch.arange(seq_len, device=self.device)
            
            # LLMå‰å‘ä¼ æ’­
            output = self.model.language_model(
                packed_query_sequence=temp_embeddings.unsqueeze(0),
                query_lens=torch.tensor([seq_len], device=self.device),
                packed_query_position_ids=position_ids,
                packed_query_indexes=torch.arange(seq_len, device=self.device),
                past_key_values=past_key_values,
                key_values_lens=torch.tensor(kv_lens, device=self.device),
                packed_key_value_indexes=kv_indexes,
                update_past_key_values=True,
                is_causal=True,
                mode="und"
            )
            
            # è·å–patchä½ç½®çš„éšè—çŠ¶æ€
            # å¤„ç†ä¸åŒçš„æ¨¡å‹è¾“å‡ºæ ¼å¼
            if hasattr(output, 'packed_query_sequence'):
                # BaseNavitOutputWithPast æ ¼å¼
                hidden_states = output.packed_query_sequence
            elif hasattr(output, 'last_hidden_state'):
                # å¦‚æœæ˜¯ä¸€ä¸ªåŒ…å« last_hidden_state å±æ€§çš„å¯¹è±¡
                hidden_states = output.last_hidden_state
            elif isinstance(output, tuple):
                # å¦‚æœæ˜¯å…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ hidden states
                hidden_states = output[0]
            else:
                # å¦åˆ™ç›´æ¥ä½¿ç”¨
                hidden_states = output
            
            patch_hidden_state = hidden_states[0, -1, :]
            
            # é¢„æµ‹velocity
            v_pred = self.model.llm2vae(patch_hidden_state.unsqueeze(0))
            
            # CFGï¼ˆå¦‚æœéœ€è¦ï¼‰
            if cfg_text_scale > 1.0 or cfg_img_scale > 1.0:
                # TODO: å®ç°CFGé€»è¾‘
                pass
            
            # æ›´æ–°x_tï¼ˆå‘clean dataæ–¹å‘ç§»åŠ¨ï¼‰
            dt = timesteps[1] - timesteps[0] if len(timesteps) > 1 else t
            x_t = x_t - v_pred * dt  # velocityæŒ‡å‘noiseåˆ°dataçš„æ–¹å‘
        
        # æœ€ç»ˆçš„clean latent
        clean_latent = x_t
        
        # å‡†å¤‡æœ€ç»ˆçš„embeddingï¼ˆä½¿ç”¨clean latentï¼Œtimestep=0ï¼‰
        timestep_final = torch.zeros(1, device=self.device)
        timestep_embed_final = self.model.time_embedder(timestep_final)
        latent_pos_embed_final = self.model.latent_pos_embed(current_patch_pos_id)
        patch_embedding_final = self.model.vae2llm(clean_latent) + timestep_embed_final + latent_pos_embed_final
        
        return {
            'embedding': patch_embedding_final.squeeze(0),  # ç§»é™¤batchç»´åº¦
            'latent': clean_latent.squeeze(0),
            'position_id': current_patch_pos_id,
            'timestep': timestep_final,
            'patch_index': patch_index
        }
    
    def _finalize_image_generation(
        self, 
        image_patches: List[Dict[str, Any]], 
        image_shape: tuple,
        cfg_text_scale: float,
        cfg_img_scale: float,
        num_timesteps: int,
        timestep_shift: float
    ) -> Image.Image:
        """
        å®Œæˆå›¾åƒç”Ÿæˆï¼Œå°†patchesåˆæˆä¸ºå®Œæ•´å›¾åƒ
        """
        print(f"ğŸ–¼ï¸  åˆæˆå›¾åƒï¼Œå…± {len(image_patches)} ä¸ªpatches")
        
        # æ”¶é›†æ‰€æœ‰patchçš„latent
        patch_latents = []
        for patch_data in image_patches:
            patch_latents.append(patch_data['latent'])
        
        if not patch_latents:
            # å¦‚æœæ²¡æœ‰patchæ•°æ®ï¼Œç”Ÿæˆéšæœºå›¾åƒ
            H, W = image_shape
            h = H // self.model.latent_downsample
            w = W // self.model.latent_downsample
            patch_dim = self.model.latent_patch_size ** 2 * self.model.latent_channel
            total_patches = h * w
            
            combined_latent = torch.randn(total_patches, patch_dim, device=self.device)
        else:
            combined_latent = torch.stack(patch_latents, dim=0)
        
        # è§£ç ä¸ºå›¾åƒ
        decoded_image = self._decode_patches_to_image(combined_latent, image_shape)
        
        return decoded_image
    
    def _decode_patches_to_image(
        self, 
        latent_patches: torch.Tensor, 
        image_shape: tuple
    ) -> Image.Image:
        """
        å°†latent patchesè§£ç ä¸ºå®Œæ•´å›¾åƒ
        """
        H, W = image_shape
        h = H // self.model.latent_downsample
        w = W // self.model.latent_downsample
        
        # é‡å¡‘ä¸ºå›¾åƒæ ¼å¼
        latent = latent_patches.reshape(
            1, h, w, 
            self.model.latent_patch_size, 
            self.model.latent_patch_size, 
            self.model.latent_channel
        )
        latent = torch.einsum("nhwpqc->nchpwq", latent)
        latent = latent.reshape(
            1, self.model.latent_channel, 
            h * self.model.latent_patch_size, 
            w * self.model.latent_patch_size
        )
        
        # VAEè§£ç 
        with torch.no_grad():
            image = self.vae_model.decode(latent)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = (image * 0.5 + 0.5).clamp(0, 1)[0].permute(1, 2, 0) * 255
        image = Image.fromarray(image.to(torch.uint8).cpu().numpy())
        
        return image
    
    def _parse_generated_sequence(
        self, 
        generated_sequence: List[Dict[str, Any]]
    ) -> List[Union[str, Image.Image]]:
        """
        è§£æç”Ÿæˆåºåˆ—ï¼Œç»„åˆæ–‡æœ¬å’Œå›¾åƒè¾“å‡º
        """
        output_results = []
        text_buffer = []
        
        for item in generated_sequence:
            item_type = item['type']
            
            if item_type == 'text_token':
                text_buffer.append(item['content'])
                
            elif item_type == 'special_token':
                # ç‰¹æ®Štokenä¸åŒ…å«åœ¨æ–‡æœ¬è¾“å‡ºä¸­ï¼Œä½†å¯ä»¥ä½œä¸ºåˆ†éš”ç¬¦
                if text_buffer and item['token_name'] in ['start_of_image']:
                    # åœ¨å›¾åƒå¼€å§‹å‰ï¼Œè¾“å‡ºç´¯ç§¯çš„æ–‡æœ¬
                    try:
                        text = self.tokenizer.decode(text_buffer, skip_special_tokens=True)
                        if text.strip():
                            output_results.append(text.strip())
                    except:
                        output_results.append("[æ–‡æœ¬è§£ç å¤±è´¥]")
                    text_buffer = []
                    
            elif item_type == 'image':
                # æ·»åŠ ç”Ÿæˆçš„å›¾åƒ
                output_results.append(item['content'])
                
            elif item_type == 'image_patch':
                # patchä¸ç›´æ¥è¾“å‡ºï¼Œç”±_finalize_image_generationå¤„ç†
                pass
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if text_buffer:
            try:
                text = self.tokenizer.decode(text_buffer, skip_special_tokens=True)
                if text.strip():
                    output_results.append(text.strip())
            except:
                output_results.append("[æ–‡æœ¬è§£ç å¤±è´¥]")
        
        return output_results


def load_bagel_model_for_inference(
    model_path: str,
    mode: int = 1,  # 1: æ­£å¸¸æ¨¡å¼, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–
    device: str = "cuda"
) -> Tuple[Bagel, Any, Qwen2Tokenizer, ImageTransform, ImageTransform, Dict[str, int]]:
    """
    æ­£ç¡®åŠ è½½BAGELæ¨¡å‹ç”¨äºæ¨ç†
    å‚è€ƒapp.pyå’Œinferencer.pyçš„åŠ è½½æ–¹å¼
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        mode: åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)
        device: è®¾å¤‡
        
    Returns:
        (model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids)
    """
    print(f"ğŸ“¦ å¼€å§‹åŠ è½½BAGELæ¨¡å‹ï¼Œè·¯å¾„: {model_path}")
    print(f"ğŸ”§ åŠ è½½æ¨¡å¼: {mode} (1=æ­£å¸¸, 2=NF4é‡åŒ–, 3=INT8é‡åŒ–)")
    
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, "llm_config.json"))
    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = "Qwen2MoTDecoderLayer"
    
    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, "vit_config.json"))
    vit_config.rope = False
    vit_config.num_hidden_layers -= 1
    
    # 2. åŠ è½½VAEæ¨¡å‹
    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, "ae.safetensors"))
    print("âœ… VAEæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. åˆ›å»ºBAGELé…ç½®
    config = BagelConfig(
        visual_gen=True,
        visual_und=True,
        llm_config=llm_config, 
        vit_config=vit_config,
        vae_config=vae_config,
        vit_max_num_patch_per_side=70,
        connector_act='gelu_pytorch_tanh',
        latent_patch_size=2,
        max_latent_size=64,
    )
    
    # 4. åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨init_empty_weightsé¿å…å†…å­˜é—®é¢˜ï¼‰
    with init_empty_weights():
        language_model = Qwen2ForCausalLM(llm_config)
        vit_model = SiglipVisionModel(vit_config)
        model = Bagel(language_model, vit_model, config)
        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)
    
    # 5. åŠ è½½tokenizerå’Œç‰¹æ®Štokens
    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)
    print("âœ… Tokenizerå’Œç‰¹æ®ŠtokensåŠ è½½å®Œæˆ")
    
    # 6. åˆ›å»ºå›¾åƒå˜æ¢
    vae_transform = ImageTransform(1024, 512, 16)
    vit_transform = ImageTransform(980, 224, 14)
    print("âœ… å›¾åƒå˜æ¢åˆ›å»ºå®Œæˆ")
    
    # 7. è®¾ç½®è®¾å¤‡æ˜ å°„
    device_map = infer_auto_device_map(
        model,
        max_memory={i: "80GiB" for i in range(torch.cuda.device_count())},
        no_split_module_classes=["Bagel", "Qwen2MoTDecoderLayer"],
    )
    
    # ç¡®ä¿ç›¸å…³æ¨¡å—åœ¨åŒä¸€è®¾å¤‡ä¸Š
    same_device_modules = [
        'language_model.model.embed_tokens',
        'time_embedder',
        'latent_pos_embed',
        'vae2llm',
        'llm2vae',
        'connector',
        'vit_pos_embed'
    ]
    
    if torch.cuda.device_count() == 1:
        first_device = device_map.get(same_device_modules[0], "cuda:0")
        for k in same_device_modules:
            if k in device_map:
                device_map[k] = first_device
            else:
                device_map[k] = "cuda:0"
    else:
        first_device = device_map.get(same_device_modules[0])
        for k in same_device_modules:
            device_map[k] = first_device
    
    # 8. æ ¹æ®æ¨¡å¼åŠ è½½æƒé‡
    if mode == 1:  # æ­£å¸¸æ¨¡å¼
        model = load_checkpoint_and_dispatch(
            model, 
            checkpoint=os.path.join(model_path, "ema.safetensors"), 
            device_map=device_map,
            offload_folder="offload",
            dtype=torch.bfloat16,
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (æ­£å¸¸æ¨¡å¼)")
        
    elif mode == 2:  # NF4é‡åŒ–
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_4bit=True, 
            bnb_4bit_compute_dtype=torch.bfloat16, 
            bnb_4bit_use_double_quant=False, 
            bnb_4bit_quant_type="nf4"
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (NF4é‡åŒ–æ¨¡å¼)")
        
    elif mode == 3:  # INT8é‡åŒ–
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_8bit=True, 
            torch_dtype=torch.float32
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (INT8é‡åŒ–æ¨¡å¼)")
        
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„åŠ è½½æ¨¡å¼: {mode}")
    
    print("ğŸ‰ BAGELæ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids


class UnifiedImageEditingInference:
    """ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“"""
    
    def __init__(
        self,
        model_path: str,
        mode: int = 1,
        device: str = "cuda"
    ):
        """
        åˆå§‹åŒ–å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“
        
        Args:
            model_path: BAGELæ¨¡å‹è·¯å¾„
            mode: åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)
            device: è®¾å¤‡
        """
        self.device = device
        self.model_path = model_path
        
        # åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
        (self.model, self.vae_model, self.tokenizer, 
         self.vae_transform, self.vit_transform, self.new_token_ids) = load_bagel_model_for_inference(
            model_path=model_path,
            mode=mode,
            device=device
        )
        
        # åˆ›å»ºæ¨ç†å™¨ï¼ˆä¿ç•™åŸæœ‰çš„å…¼å®¹æ€§ï¼‰
        self.inferencer = InterleaveInferencer(
            model=self.model,
            vae_model=self.vae_model,
            tokenizer=self.tokenizer,
            vae_transform=self.vae_transform,
            vit_transform=self.vit_transform,
            new_token_ids=self.new_token_ids
        )
        
        # åˆ›å»ºç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨ï¼ˆæ–°å¢ï¼‰
        self.unified_inferencer = UnifiedAutoregressiveInferencer(
            model=self.model,
            vae_model=self.vae_model,
            tokenizer=self.tokenizer,
            vae_transform=self.vae_transform,
            vit_transform=self.vit_transform,
            new_token_ids=self.new_token_ids
        )
        
        print("ğŸš€ å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
    
    def edit_image(
        self,
        image_path: str,
        edit_prompt: str,
        think: bool = True,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        å›¾åƒç¼–è¾‘çš„ä¸»è¦æ¥å£
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            think: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
        """
        print(f"ğŸ–¼ï¸  å¼€å§‹å›¾åƒç¼–è¾‘")
        print(f"ğŸ“¸ è¾“å…¥å›¾åƒ: {image_path}")
        print(f"âœï¸  ç¼–è¾‘æç¤º: {edit_prompt}")
        
        # åŠ è½½è¾“å…¥å›¾åƒ
        try:
            input_image = Image.open(image_path).convert('RGB')
            print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {input_image.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return [f"å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        # æ„å»ºè¾“å…¥åˆ—è¡¨
        input_lists = [input_image, edit_prompt]
        
        # æ‰§è¡Œæ¨ç†
        try:
            results = self.inferencer.interleave_inference(
                input_lists=input_lists,
                think=think,
                understanding_output=False,  # ç¼–è¾‘æ¨¡å¼ï¼Œä¸æ˜¯ç†è§£æ¨¡å¼
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
                image_shapes=image_shapes,
                **kwargs
            )
            
            print(f"âœ… ç¼–è¾‘å®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ å›¾åƒç¼–è¾‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"å›¾åƒç¼–è¾‘å¤±è´¥: {e}"]
    
    def unified_edit_image(
        self,
        image_path: str,
        edit_prompt: str,
        use_autoregressive: bool = True,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 0.8,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¥å£ - æ”¯æŒçœŸæ­£çš„è‡ªå›å½’äº¤é”™æ¨ç†
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼ï¼ˆå¯¹åº”è®­ç»ƒï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
        """
        print(f"ğŸ¨ å¼€å§‹ç»Ÿä¸€å›¾åƒç¼–è¾‘")
        print(f"ğŸ“¸ è¾“å…¥å›¾åƒ: {image_path}")
        print(f"âœï¸  ç¼–è¾‘æç¤º: {edit_prompt}")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        # åŠ è½½è¾“å…¥å›¾åƒ
        try:
            input_image = Image.open(image_path).convert('RGB')
            print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {input_image.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return [f"å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        try:
            if use_autoregressive:
                # ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†ï¼ˆå¯¹åº”è®­ç»ƒé€»è¾‘ï¼‰
                print("ğŸš€ ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†æ¨¡å¼")
                results = self.unified_inferencer.unified_autoregressive_inference(
                    input_text=edit_prompt,
                    input_image=input_image,
                    max_length=max_length,
                    do_sample=do_sample,
                    temperature=temperature,
                    image_shapes=image_shapes,
                    cfg_text_scale=cfg_text_scale,
                    cfg_img_scale=cfg_img_scale,
                    num_timesteps=num_timesteps,
                    timestep_shift=3.0,  # æ·»åŠ é»˜è®¤å€¼
                    **kwargs
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†ï¼ˆå…¼å®¹æ€§ï¼‰
                print("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†æ¨¡å¼")
                input_lists = [input_image, edit_prompt]
                results = self.inferencer.interleave_inference(
                    input_lists=input_lists,
                    understanding_output=False,
                    cfg_text_scale=cfg_text_scale,
                    cfg_img_scale=cfg_img_scale,
                    num_timesteps=num_timesteps,
                    image_shapes=image_shapes,
                    **kwargs
                )
            
            print(f"âœ… å›¾åƒç¼–è¾‘å®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ ç»Ÿä¸€å›¾åƒç¼–è¾‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"ç»Ÿä¸€å›¾åƒç¼–è¾‘å¤±è´¥: {e}"]
    
    def autoregressive_multi_modal_generation(
        self,
        prompt: str,
        input_image: Optional[str] = None,
        force_image_generation: bool = False,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 0.8,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ - å®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„ç»Ÿä¸€åºåˆ—å»ºæ¨¡
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            input_image: å¯é€‰çš„è¾“å…¥å›¾åƒè·¯å¾„
            force_image_generation: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå›¾åƒ
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆæ–‡æœ¬å’Œå›¾åƒäº¤é”™ï¼‰
        """
        print(f"ğŸ¯ å¼€å§‹è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ")
        print(f"ğŸ“ è¾“å…¥æç¤º: {prompt}")
        print(f"ğŸ–¼ï¸  è¾“å…¥å›¾åƒ: {input_image if input_image else 'None'}")
        print(f"ğŸ”§ å¼ºåˆ¶å›¾åƒç”Ÿæˆ: {force_image_generation}")
        
        # å¤„ç†è¾“å…¥å›¾åƒ
        image_obj = None
        if input_image:
            try:
                image_obj = Image.open(input_image).convert('RGB')
                print(f"âœ… è¾“å…¥å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {image_obj.size}")
            except Exception as e:
                print(f"âŒ è¾“å…¥å›¾åƒåŠ è½½å¤±è´¥: {e}")
                return [f"è¾“å…¥å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        if force_image_generation and '<|vision_start|>' not in prompt:
            # å¼ºåˆ¶æ·»åŠ å›¾åƒç”Ÿæˆtoken
            enhanced_prompt = f"{prompt} <|vision_start|> <|vision_end|>"
            print(f"ğŸ”§ å¼ºåˆ¶å›¾åƒç”Ÿæˆï¼Œå¢å¼ºæç¤º: {enhanced_prompt}")
        else:
            enhanced_prompt = prompt
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†
            results = self.unified_inferencer.unified_autoregressive_inference(
                input_text=enhanced_prompt,
                input_image=image_obj,
                max_length=max_length,
                do_sample=do_sample,
                temperature=temperature,
                image_shapes=image_shapes,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
                **kwargs
            )
            
            print(f"âœ… è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {e}"]
    
    def batch_edit_images(
        self,
        image_paths: List[str],
        edit_prompts: List[str],
        output_dir: str = "edited_images",
        use_autoregressive: bool = True,
        **kwargs
    ) -> Dict[str, List[Union[str, Image.Image]]]:
        """
        æ‰¹é‡å›¾åƒç¼–è¾‘
        
        Args:
            image_paths: è¾“å…¥å›¾åƒè·¯å¾„åˆ—è¡¨
            edit_prompts: ç¼–è¾‘æç¤ºè¯åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘ç»“æœå­—å…¸
        """
        print(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å›¾åƒç¼–è¾‘ï¼Œå…± {len(image_paths)} å¼ å›¾åƒ")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        os.makedirs(output_dir, exist_ok=True)
        results = {}
        
        for i, (image_path, edit_prompt) in enumerate(zip(image_paths, edit_prompts)):
            print(f"\nğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(image_paths)} å¼ å›¾åƒ")
            
            # å•å¼ å›¾åƒç¼–è¾‘ - ä½¿ç”¨ç»Ÿä¸€æ¥å£
            if use_autoregressive:
                edit_results = self.unified_edit_image(
                    image_path=image_path,
                    edit_prompt=edit_prompt,
                    use_autoregressive=True,
                    **kwargs
                )
            else:
                edit_results = self.edit_image(
                    image_path=image_path,
                    edit_prompt=edit_prompt,
                    **kwargs
                )
            
            # ä¿å­˜ç»“æœ
            results[f"image_{i+1}"] = edit_results
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
            for j, result in enumerate(edit_results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(output_dir, f"image_{i+1}_result_{j}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(output_dir, f"image_{i+1}_text_{j}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}")
        
        print(f"ğŸ‰ æ‰¹é‡ç¼–è¾‘å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")
        return results
    
    def multi_step_edit(
        self,
        image_path: str,
        edit_steps: List[str],
        output_dir: str = "multi_step_edit",
        use_autoregressive: bool = True,
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        å¤šæ­¥éª¤å›¾åƒç¼–è¾‘
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_steps: ç¼–è¾‘æ­¥éª¤åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ‰€æœ‰æ­¥éª¤çš„ç»“æœ
        """
        print(f"ğŸ¯ å¼€å§‹å¤šæ­¥éª¤å›¾åƒç¼–è¾‘ï¼Œå…± {len(edit_steps)} ä¸ªæ­¥éª¤")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        os.makedirs(output_dir, exist_ok=True)
        all_results = []
        current_image_path = image_path
        
        for i, edit_step in enumerate(edit_steps):
            print(f"\nğŸ”„ æ‰§è¡Œæ­¥éª¤ {i+1}/{len(edit_steps)}: {edit_step}")
            
            # æ‰§è¡Œå½“å‰æ­¥éª¤ - ä½¿ç”¨ç»Ÿä¸€æ¥å£
            if use_autoregressive:
                step_results = self.unified_edit_image(
                    image_path=current_image_path,
                    edit_prompt=edit_step,
                    use_autoregressive=True,
                    **kwargs
                )
            else:
                step_results = self.edit_image(
                    image_path=current_image_path,
                    edit_prompt=edit_step,
                    **kwargs
                )
            
            all_results.extend(step_results)
            
            # ä¿å­˜å½“å‰æ­¥éª¤ç»“æœ
            step_dir = os.path.join(output_dir, f"step_{i+1}")
            os.makedirs(step_dir, exist_ok=True)
            
            for j, result in enumerate(step_results):
                if isinstance(result, Image.Image):
                    result_path = os.path.join(step_dir, f"result_{j}.png")
                    result.save(result_path)
                    print(f"ğŸ’¾ æ­¥éª¤ {i+1} å›¾åƒå·²ä¿å­˜: {result_path}")
                    
                    # æ›´æ–°current_image_pathä¸ºæœ€æ–°ç”Ÿæˆçš„å›¾åƒï¼ˆç”¨äºä¸‹ä¸€æ­¥ï¼‰
                    current_image_path = result_path
                    
                elif isinstance(result, str):
                    result_path = os.path.join(step_dir, f"text_{j}.txt")
                    with open(result_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ æ­¥éª¤ {i+1} æ–‡æœ¬å·²ä¿å­˜: {result_path}")
        
        print(f"ğŸ‰ å¤šæ­¥éª¤ç¼–è¾‘å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")
        return all_results


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="BAGELç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†")
    parser.add_argument("--model_path", type=str, required=True, help="BAGELæ¨¡å‹è·¯å¾„")
    parser.add_argument("--image_path", type=str, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--edit_prompt", type=str, help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--prompt", type=str, help="é€šç”¨æç¤ºè¯ï¼ˆç”¨äºå¤šæ¨¡æ€ç”Ÿæˆï¼‰")
    parser.add_argument("--output_dir", type=str, default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--mode", type=int, default=1, help="æ¨¡å‹åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)")
    
    # æ¨ç†æ¨¡å¼é€‰æ‹©
    parser.add_argument("--use_autoregressive", action="store_true", default=True, 
                      help="ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†æ¨¡å¼ï¼ˆå¯¹åº”è®­ç»ƒï¼‰")
    parser.add_argument("--use_legacy", action="store_true", 
                      help="ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†æ¨¡å¼ï¼ˆå…¼å®¹æ€§ï¼‰")
    parser.add_argument("--generation_mode", type=str, choices=["edit", "generate", "multi_modal"], 
                      default="edit", help="ç”Ÿæˆæ¨¡å¼ï¼šedit=å›¾åƒç¼–è¾‘, generate=çº¯ç”Ÿæˆ, multi_modal=å¤šæ¨¡æ€ç”Ÿæˆ")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_length", type=int, default=500, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--do_sample", action="store_true", default=True, help="æ˜¯å¦é‡‡æ ·")
    parser.add_argument("--temperature", type=float, default=0.8, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--force_image_generation", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆå›¾åƒ")
    
    parser.add_argument("--think", action="store_true", help="å¯ç”¨æ€è€ƒæ¨¡å¼")
    parser.add_argument("--cfg_text_scale", type=float, default=3.0, help="æ–‡æœ¬CFGå¼ºåº¦")
    parser.add_argument("--cfg_img_scale", type=float, default=1.5, help="å›¾åƒCFGå¼ºåº¦")
    parser.add_argument("--num_timesteps", type=int, default=50, help="å»å™ªæ­¥æ•°")
    parser.add_argument("--multi_step", nargs="+", help="å¤šæ­¥éª¤ç¼–è¾‘ï¼ˆæä¾›å¤šä¸ªç¼–è¾‘æ­¥éª¤ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # å¤„ç†æ¨ç†æ¨¡å¼
    use_autoregressive = args.use_autoregressive and not args.use_legacy
    
    print("ğŸš€ åˆå§‹åŒ–BAGELç»Ÿä¸€æ¨ç†å¼•æ“...")
    print(f"ğŸ”§ æ¨ç†æ¨¡å¼: {'ç»Ÿä¸€è‡ªå›å½’' if use_autoregressive else 'ä¼ ç»Ÿäº¤é”™'}")
    print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼: {args.generation_mode}")
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“
        inference_engine = UnifiedImageEditingInference(
            model_path=args.model_path,
            mode=args.mode
        )
        
        os.makedirs(args.output_dir, exist_ok=True)
        
        # æ ¹æ®ç”Ÿæˆæ¨¡å¼æ‰§è¡Œä¸åŒçš„é€»è¾‘
        if args.generation_mode == "edit":
            # å›¾åƒç¼–è¾‘æ¨¡å¼
            if not args.image_path or not args.edit_prompt:
                print("âŒ å›¾åƒç¼–è¾‘æ¨¡å¼éœ€è¦ --image_path å’Œ --edit_prompt å‚æ•°")
                return
                
            if args.multi_step:
                # å¤šæ­¥éª¤ç¼–è¾‘
                print("ğŸ¯ æ‰§è¡Œå¤šæ­¥éª¤å›¾åƒç¼–è¾‘")
                results = inference_engine.multi_step_edit(
                    image_path=args.image_path,
                    edit_steps=args.multi_step,
                    output_dir=args.output_dir,
                    use_autoregressive=use_autoregressive,
                    max_length=args.max_length,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    cfg_text_scale=args.cfg_text_scale,
                    cfg_img_scale=args.cfg_img_scale,
                    num_timesteps=args.num_timesteps
                )
            else:
                # å•æ­¥ç¼–è¾‘
                print("âœï¸  æ‰§è¡Œå•æ­¥å›¾åƒç¼–è¾‘")
                results = inference_engine.unified_edit_image(
                    image_path=args.image_path,
                    edit_prompt=args.edit_prompt,
                    use_autoregressive=use_autoregressive,
                    max_length=args.max_length,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    cfg_text_scale=args.cfg_text_scale,
                    cfg_img_scale=args.cfg_img_scale,
                    num_timesteps=args.num_timesteps
                )
                
                # ä¿å­˜ç»“æœ
                for i, result in enumerate(results):
                    if isinstance(result, Image.Image):
                        output_path = os.path.join(args.output_dir, f"edited_image_{i}.png")
                        result.save(output_path)
                        print(f"ğŸ’¾ ç¼–è¾‘ç»“æœå·²ä¿å­˜: {output_path}")
                    elif isinstance(result, str):
                        output_path = os.path.join(args.output_dir, f"text_result_{i}.txt")
                        with open(output_path, "w", encoding="utf-8") as f:
                            f.write(result)
                        print(f"ğŸ’¾ æ–‡æœ¬ç»“æœå·²ä¿å­˜: {output_path}")
                        
        elif args.generation_mode == "multi_modal":
            # å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼
            if not args.prompt:
                print("âŒ å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼éœ€è¦ --prompt å‚æ•°")
                return
                
            print("ğŸ¯ æ‰§è¡Œè‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ")
            results = inference_engine.autoregressive_multi_modal_generation(
                prompt=args.prompt,
                input_image=args.image_path,
                force_image_generation=args.force_image_generation,
                max_length=args.max_length,
                do_sample=args.do_sample,
                temperature=args.temperature,
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.num_timesteps
            )
            
            # ä¿å­˜ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(args.output_dir, f"generated_image_{i}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ ç”Ÿæˆå›¾åƒå·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(args.output_dir, f"generated_text_{i}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ ç”Ÿæˆæ–‡æœ¬å·²ä¿å­˜: {output_path}")
                    
        elif args.generation_mode == "generate":
            # çº¯ç”Ÿæˆæ¨¡å¼ï¼ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¯¹æ¯”ï¼‰
            if not args.prompt:
                print("âŒ ç”Ÿæˆæ¨¡å¼éœ€è¦ --prompt å‚æ•°")
                return
                
            print("ğŸ¯ æ‰§è¡Œä¼ ç»Ÿæ¨ç†ç”Ÿæˆ")
            if args.image_path:
                input_image = Image.open(args.image_path).convert('RGB')
                input_lists = [input_image, args.prompt]
            else:
                input_lists = [args.prompt]
                
            results = inference_engine.inferencer.interleave_inference(
                input_lists=input_lists,
                think=args.think,
                understanding_output=False,
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.num_timesteps
            )
            
            # ä¿å­˜ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(args.output_dir, f"traditional_image_{i}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ ä¼ ç»Ÿç”Ÿæˆå›¾åƒå·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(args.output_dir, f"traditional_text_{i}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ ä¼ ç»Ÿç”Ÿæˆæ–‡æœ¬å·²ä¿å­˜: {output_path}")
        
        print("ğŸ‰ æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
